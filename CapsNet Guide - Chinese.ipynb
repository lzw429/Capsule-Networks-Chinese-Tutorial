{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# 胶囊网络指南\n",
    "\n",
    "本文参考来自 Kaggle 社区的英文[指南](https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-capsule-networks/notebook)，翻译并补充内容。\n",
    "\n",
    "本笔记本使用 Keras 介绍并实现胶囊网络 (CapsNet)，并在 DonorsChoose.Org 的[竞赛](https://www.kaggle.com/c/donorschoose-application-screening/overview)中评估了其性能。\n",
    "文中提供的示例仅用于促进对模型的理解。\n",
    "\n",
    "在 Kaggle Kernel 中运行时，因为获取数据集的需要可能需要**开启互联网连接**。\n",
    "\n",
    "胶囊网络最初在 2017 年由 Hinton 等人的《Dynamic Routing Between Capsules》提出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录\n",
    "\n",
    "- 胶囊网络介绍\n",
    "    - 人类视觉识别\n",
    "    - 动机：CNN 的缺陷\n",
    "    - 胶囊\n",
    "    - 按一致路由\n",
    "    - 胶囊网络的数学推导\n",
    "    - 动态路由算法\n",
    "    - 关于压缩函数\n",
    "    - 胶囊网络的优势\n",
    "- 样例代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 胶囊网络介绍\n",
    "\n",
    "### 人类视觉识别\n",
    "\n",
    "任何现实的对象都可以认为是由几个较小的对象组成。例如，一棵树由树干，树冠和根组成。这些部分形成层次结构。一棵树的树冠进一步由树枝组成，树枝上有叶子。\n",
    "\n",
    "![Parts of a tree](https://raw.githubusercontent.com/zaffnet/images/master/images/tree.png)\n",
    "\n",
    "每当我们看到某个物体时，我们的眼睛就会注视一些固定点，这些固定点的相对位置和性质有助于我们的大脑识别该物体。这样，我们的大脑不必处理所有细节。仅仅看到一些树叶和树枝，我们的大脑就会意识到树冠。冠立在树干上，树干下面有一些根。结合这些分层信息，我们的大脑就会意识到有一棵树。由此，我们将对象的各个部分称为实体。\n",
    "\n",
    "### 动机：CNN 的缺陷\n",
    "\n",
    "\n",
    "\n",
    "### 胶囊\n",
    "\n",
    "胶囊网络模型背后的假设是，存在胶囊（一组神经元）来告诉图像中是否存在某些对象（**实体**）。对应于每个实体，都有一个胶囊，它给出：\n",
    "\n",
    "- 实体存在于图像中的概率\n",
    "- 该实体的**实例化参数**(instantiation parameters)\n",
    "\n",
    "实例化参数是图像中该实体的属性（例如“位置”、“大小”、“色相”等）。例如，矩形是一个简单的几何对象。对应于矩形的胶囊将告诉我们其实例化参数。\n",
    "\n",
    "![Rectangle capsule](https://raw.githubusercontent.com/zaffnet/images/master/images/rectangle.png)\n",
    "\n",
    "从上图中可见，我们假想的胶囊由 6 个神经元组成，每个神经元对应于矩形的某些属性，一组神经元的值对应一个向量，称为**激活向量**。该向量的长度表示的是矩阵存在的可能性。因此，矩形存在的可能性可写作：\n",
    "\n",
    "$$\n",
    "\\sqrt{1.3^{2} + 0.6^{2} + 7.4^{2} + 6.5^{2} + 0.5^{2} + 1.4^{2}} = 10.06\n",
    "$$\n",
    "\n",
    "但是，如果使用概率来表示可能性，这个值应该在 0 到 1 的范围内。因此胶囊的输出需要进行如下转换：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{v}_{j}=\\frac{\\left\\|\\mathbf{s}_{j}\\right\\|^{2}}{1+\\left\\|\\mathbf{s}_{j}\\right\\|^{2}}  \\frac{\\mathbf{s}_{j}}{\\left\\|\\mathbf{s}_{j}\\right\\|}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "这个非线性变换被称为压缩函数 (squashing function)，它充当胶囊网络的激活函数，如果 CNN 中使用的 ReLU。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 按一致路由\n",
    "\n",
    "一个胶囊网络由若干层组成。低层的胶囊对应于简单的实体（例如，矩形、三角形）。这些低级的胶囊推断存在更复杂的实体，并且它们的推断被“组合”以产生高级胶囊（例如，房屋的门、窗）的输出。例如，存在一个*矩形*（x轴角度 = 0，大小 = 5，位置 = 0，...）和*三角形*（x轴角度 = 6，大小 = 5，位置 = 5，...）共同推断一座*房屋*的存在（更高层的实体）。\n",
    "\n",
    "需要注意的是，胶囊网络中，胶囊之间可能有**耦合作用** (coupling effect)。当一个低层级的胶囊成功学习到一个高层级实体时，该实体对应的高层级胶囊将反馈发送到这些低层级胶囊，从而增加其在该高级别胶囊上的推测。为理解这一点，我们假设有这样两层胶囊：\n",
    "\n",
    "- 低层对应于矩形，三角形和圆形\n",
    "- 高层对应于房屋，船只和汽车\n",
    "\n",
    "如果一个图像包含房屋，则网络中对应于矩形和三角形的胶囊将具有较大的激活向量。它们的相对位置（编码在其实例化参数中）将取决于高级对象。由于它们将就房屋的存在达成一致，房屋胶囊的输出向量将变大。反过来，这将使矩形和三角形胶囊所得的预测值更大。此循环将重复 4-5 次，之后对房屋存在的推测概率将比对船或汽车存在的推测概率要大得多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 胶囊网络的数学推导\n",
    "\n",
    "假设第 $l$ 层和第 $l + 1$ 层分别有 $m$ 和 $n$ 个胶囊，我们的任务是根据第 $l$ 层的激活值计算第 $l + 1$ 层的激活值。令 $u$ 表示第 $l$ 层的激活值，我们计算的是第 $l + 1$ 层的激活值 $v$。\n",
    "\n",
    "对于第 $l + 1$ 层的胶囊 $j$：\n",
    "\n",
    "(1) 首先通过第 $l$ 层计算**预测向量**。第 $l$ 层胶囊 $i$ 提供给第 $l + 1$ 层的胶囊 $j$ 的预测向量是：\n",
    "$$\n",
    "\\hat{\\mathbf{u}_{j | i}}=\\mathbf{W}_{i j} \\mathbf{u}_{i}\n",
    "$$\n",
    "$\\mathbf{W}_{i j}$ 是待学习的权重矩阵。\n",
    "\n",
    "(2) 然后计算胶囊 $j$ 的**输出向量**。输出向量是第 $l$ 层胶囊对胶囊 $j$ 给出的所有预测向量的加权和。\n",
    "$$\n",
    "s_{j}=\\sum_{i=1}^{m} c_{i j} \\hat{\\mathbf{u}_{j | i}}\n",
    "$$\n",
    "标量 $\\mathbf{c}_{ij}$ 被称为第 $l$ 层胶囊 $i$ 和第 $l + 1$ 层胶囊 $j$ 之间的耦合系数。这些系数由**迭代动态路由算法**确定。\n",
    "\n",
    "(3) 对输出向量应用压缩函数，来获得胶囊 $j$ 的激活向量 $\\mathbf{v}_j$：\n",
    "$$\n",
    "\\mathbf{v}_{j}=\\text{ squash}\\left(\\mathbf{s}_{j}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动态路由算法 The dynamic routing algorithm\n",
    "\n",
    "路由算法 The dynamic routing algorithm\n",
    "\n",
    "由 CNN 的缺陷得到启发，胶囊的概念并不复杂。构建胶囊网络的关键在于实现胶囊之间的耦合作用，动态路由算法解决了这一问题。\n",
    "\n",
    "第 $l + 1$ 层的激活向量发送反馈信号到第 $l$ 层的胶囊。如果第 $l$ 层胶囊 $i$ 和第 $l + 1$ 层胶囊 $j$ 在胶囊 $j$ 的激活向量上的推测一致，它们的点积(dot product)值应该比较高。因此，预测向量 $\\hat{\\mathbf{u}_{j | i}}$ 的“宽度”在 $j$ 的输出中应该提升。换句话说，那些有助于激活向量的预测向量在输出向量（以及后续的激活向量）中具有更多的权重。这个互助周期持续 4-5 轮。\n",
    "\n",
    "但是低层胶囊对高层胶囊的预测应该被加和到一个值，也就是为什么对于第 $l$ 层胶囊 $i$ 有：\n",
    "\n",
    "$$\n",
    "c_{i j}=\\frac{\\exp \\left(b_{i j}\\right)}{\\sum_{k} \\exp \\left(b_{i k}\\right)}\n",
    "$$\n",
    "\n",
    "显然，\n",
    "\n",
    "$$\n",
    "\\sum_{k} c_{i k}=1\n",
    "$$\n",
    "\n",
    "其中，$c_{ij}$ 如前文所述，是两胶囊之间的耦合系数。logit $b_{ij}$ 表示第 $l$ 层胶囊 $i$ 和第 $l + 1$ 层胶囊 $j$ 是否有强耦合。换句话说，它是对胶囊 $j$ 的存在有多少程度是被胶囊 $i$ 解释的度量。所有 $b_ij$ 的初始值应该是相等的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 给定预测向量 $\\hat{\\mathbf{u}_{j | i}}$，路由迭代次数 $r$，\n",
    ">\n",
    "> 对所有第 $l$ 层胶囊 $i$ 和第 $l + 1$ 层胶囊 $j$，$b_{ij} = 0$。\n",
    ">\n",
    "> 对于 $r$ 次迭代：\n",
    ">\n",
    ">> 对第 $l$ 层的所有胶囊 $i$：$c_i=\\text{softmax}(b_i)$ 一个胶囊对高层胶囊的推测值的和应为 1\n",
    ">>\n",
    ">> 对第 $l + 1$ 层的所有胶囊 $j$：$s_{j}=\\sum_{i=1}^{m} c_{i j} \\mathbf{u}_{j i}$，输出向量是预测向量的加权和\n",
    ">>\n",
    ">> 对第 $l + 1$ 层的所有胶囊 $j$：$\\mathbf{v}_{j}=\\operatorname{squash}\\left(\\mathbf{s}_{j}\\right)$，应用激活函数\n",
    ">>\n",
    ">> 对所有第 $l$ 层胶囊 $i$ 和第 $l + 1$ 层胶囊 $j$：$b_{ij} = b_{ij} + \\hat{\\mathbf{u}_{j | i}} \\cdot \\mathbf{v}_j $\n",
    ">\n",
    "> 返回 $\\mathbf{v}_j$\n",
    "\n",
    "循环中的最后一行至关重要，路由就依此实现。如果积 $\\hat{\\mathbf{u}_{j | i}} \\cdot \\mathbf{v}_j$ 很大，它将增大 $b_{ij}$，这将进一步增大对应的耦合系数 $c_{ij}$，反过来会使 $\\hat{\\mathbf{u}_{j | i}} \\cdot \\mathbf{v}_j$ 变得更大。\n",
    "\n",
    "这是胶囊网络工作的原理。了解这一点之后，你应该能没有困难地直接阅读[原论文](https://arxiv.org/pdf/1710.09829.pdf)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于压缩函数\n",
    "当 $\\|\\mathbf{s}\\| = 0$ 时，$\\|\\mathbf{s}\\| $ 的导数是为定义的，它在训练期间可能会发生梯度爆炸。如果一个向量是零向量，它的梯度会是 `NaN`，所以当优化器更新这个变量时，它会变为 `NaN`。解决方案是通过计算平方和加上很小的 epsilon 值的平方根来手动计算范数：$\\|\\mathbf{s}\\|\\approx \\sqrt{\\sum_{i} s_{i}^{2}+\\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 胶囊网络的优势\n",
    "\n",
    "CNN 当中包含池化层，我们通常使用最大池化来作为一种最原始的路由机制。一个局部池中，最活跃的特征会被路由到更高层，而高层的检测器在路由过程中没有任何反馈。相比于胶囊网络按一致路由(routing by agreement)的机制，只有与高层检测器达成一致的特征才会被路由。这是胶囊网络相比于 CNN 的关键优势，它具有出色的动态路由机制。称为动态，是因为需要路由的信息是实时确定的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import nltk\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download(\"punkt\") # 载入该数据可能需要互联网连接！\n",
    "# nltk 自然语言工具包，用于以 Python 编程语言编写的英语的符号和统计自然语言处理。包括图形演示和样本数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences, words_dict):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in tqdm.tqdm(sentences):\n",
    "        if hasattr(sentence, \"decode\"):\n",
    "            sentence = sentence.decode(\"utf-8\")\n",
    "        tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "        result = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = len(words_dict)\n",
    "            word_index = words_dict[word]\n",
    "            result.append(word_index)\n",
    "        tokenized_sentences.append(result)\n",
    "    return tokenized_sentences, words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_embedding_list(file_path):\n",
    "    embedding_word_dict = {}\n",
    "    embedding_list = []\n",
    "    f = open(file_path)\n",
    "\n",
    "    for index, line in enumerate(f):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            continue\n",
    "        embedding_list.append(coefs)\n",
    "        embedding_word_dict[word] = len(embedding_word_dict)\n",
    "    f.close()\n",
    "    embedding_list = np.array(embedding_list)\n",
    "    return embedding_list, embedding_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_embedding_list(embedding_list, embedding_word_dict, words_dict):\n",
    "    cleared_embedding_list = []\n",
    "    cleared_embedding_word_dict = {}\n",
    "\n",
    "    for word in words_dict:\n",
    "        if word not in embedding_word_dict:\n",
    "            continue\n",
    "        word_id = embedding_word_dict[word]\n",
    "        row = embedding_list[word_id]\n",
    "        cleared_embedding_list.append(row)\n",
    "        cleared_embedding_word_dict[word] = len(cleared_embedding_word_dict)\n",
    "\n",
    "    return cleared_embedding_list, cleared_embedding_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_tokens_to_ids(tokenized_sentences, words_list, embedding_word_dict, sentences_length):\n",
    "    words_train = []\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        current_words = []\n",
    "        for word_index in sentence:\n",
    "            word = words_list[word_index]\n",
    "            word_id = embedding_word_dict.get(word, len(embedding_word_dict) - 2)\n",
    "            current_words.append(word_id)\n",
    "\n",
    "        if len(current_words) >= sentences_length:\n",
    "            current_words = current_words[:sentences_length]\n",
    "        else:\n",
    "            current_words += [len(embedding_word_dict) - 1] * (sentences_length - len(current_words))\n",
    "        words_train.append(current_words)\n",
    "    return words_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 胶囊网络模型\n",
    "\n",
    "除了增加了一个胶囊层，胶囊网络的架构与一般深度学习模型的架构非常相似。\n",
    "在模型中使用胶囊层，而消除了池化层的存在。在许多情况下，这是比较理想的，因为模型保持了平移不变性，同时没有丢失细节。\n",
    "\n",
    "![Text Classification](https://raw.githubusercontent.com/zaffnet/images/master/images/comparison.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten\n",
    "from keras.layers import concatenate, GRU, Input, K, LSTM, MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 胶囊网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru_len = 128\n",
    "Routings = 5\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.3\n",
    "rate_drop_dense = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 胶囊网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(embedding_matrix, sequence_length, dropout_rate, recurrent_units, dense_size):\n",
    "    input1 = Input(shape=(sequence_length,))\n",
    "    embed_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix], trainable=False)(input1)\n",
    "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "    x = Bidirectional(\n",
    "        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
    "        embed_layer)\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(x)\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    output = Dense(1, activation='sigmoid')(capsule)\n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _train_model(model, batch_size, train_x, train_y, val_x, val_y):\n",
    "    num_labels = train_y.shape[1]\n",
    "    patience = 5\n",
    "    best_loss = -1\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "    \n",
    "    current_epoch = 0\n",
    "    \n",
    "    while True:\n",
    "        model.fit(train_x, train_y, batch_size=batch_size, epochs=1)\n",
    "        y_pred = model.predict(val_x, batch_size=batch_size)\n",
    "\n",
    "        total_loss = 0\n",
    "        for j in range(num_labels):\n",
    "            loss = log_loss(val_y[:, j], y_pred[:, j])\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= num_labels\n",
    "\n",
    "        print(\"Epoch {0} loss {1} best_loss {2}\".format(current_epoch, total_loss, best_loss))\n",
    "\n",
    "        current_epoch += 1\n",
    "        if total_loss < best_loss or best_loss == -1:\n",
    "            best_loss = total_loss\n",
    "            best_weights = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            if current_epoch - best_epoch == patience:\n",
    "                break\n",
    "\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_folds(X, y, X_test, fold_count, batch_size, get_model_func):\n",
    "    print(\"=\"*75)\n",
    "    fold_size = len(X) // fold_count\n",
    "    models = []\n",
    "    result_path = \"predictions\"\n",
    "    if not os.path.exists(result_path):\n",
    "        os.mkdir(result_path)\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(X)\n",
    "\n",
    "        train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = np.array(X[fold_start:fold_end])\n",
    "        val_y = np.array(y[fold_start:fold_end])\n",
    "\n",
    "        model = _train_model(get_model_func(), batch_size, train_x, train_y, val_x, val_y)\n",
    "        train_predicts_path = os.path.join(result_path, \"train_predicts{0}.npy\".format(fold_id))\n",
    "        test_predicts_path = os.path.join(result_path, \"test_predicts{0}.npy\".format(fold_id))\n",
    "        train_predicts = model.predict(X, batch_size=512, verbose=1)\n",
    "        test_predicts = model.predict(X_test, batch_size=512, verbose=1)\n",
    "        np.save(train_predicts_path, train_predicts)\n",
    "        np.save(test_predicts_path, test_predicts)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_file_path = \"../input/donorschooseorg-preprocessed-data/train_preprocessed.csv\"\n",
    "train_file_path = \"../input/donorschooseorg-preprocessed-data/train_small.csv\"\n",
    "\n",
    "# test_file_path = \"../input/donorschooseorg-preprocessed-data/test_preprocessed.csv\"\n",
    "test_file_path = \"../input/donorschooseorg-preprocessed-data/test_small.csv\"\n",
    "\n",
    "# embedding_path = \"../input/fatsttext-common-crawl/crawl-300d-2M/crawl-300d-2M.vec\"\n",
    "embedding_path = \"../input/donorschooseorg-preprocessed-data/embeddings_small.vec\"\n",
    "\n",
    "batch_size = 128 # 256\n",
    "recurrent_units = 16 # 64\n",
    "dropout_rate = 0.3 \n",
    "dense_size = 8 # 32\n",
    "sentences_length = 10 # 300\n",
    "fold_count = 2 # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNKNOWN_WORD = \"_UNK_\"\n",
    "END_WORD = \"_END_\"\n",
    "NAN_WORD = \"_NAN_\"\n",
    "CLASSES = [\"project_is_approved\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv(train_file_path)   # 训练数据\n",
    "test_data = pd.read_csv(test_file_path)     # 测试数据\n",
    "list_sentences_train = train_data[\"application_text\"].fillna(NAN_WORD).values\n",
    "list_sentences_test = test_data[\"application_text\"].fillna(NAN_WORD).values\n",
    "y_train = train_data[CLASSES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Tokenizing sentences in train set...\")\n",
    "tokenized_sentences_train, words_dict = tokenize_sentences(list_sentences_train, {})\n",
    "print(\"Tokenizing sentences in test set...\")\n",
    "tokenized_sentences_test, words_dict = tokenize_sentences(list_sentences_test, words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embedding\n",
    "words_dict[UNKNOWN_WORD] = len(words_dict)\n",
    "print(\"Loading embeddings...\")\n",
    "embedding_list, embedding_word_dict = read_embedding_list(embedding_path)\n",
    "embedding_size = len(embedding_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Preparing data...\")\n",
    "embedding_list, embedding_word_dict = clear_embedding_list(embedding_list, embedding_word_dict, words_dict)\n",
    "\n",
    "embedding_word_dict[UNKNOWN_WORD] = len(embedding_word_dict)\n",
    "embedding_list.append([0.] * embedding_size)\n",
    "embedding_word_dict[END_WORD] = len(embedding_word_dict)\n",
    "embedding_list.append([-1.] * embedding_size)\n",
    "\n",
    "embedding_matrix = np.array(embedding_list)\n",
    "\n",
    "id_to_word = dict((id, word) for word, id in words_dict.items())\n",
    "train_list_of_token_ids = convert_tokens_to_ids(\n",
    "    tokenized_sentences_train,\n",
    "    id_to_word,\n",
    "    embedding_word_dict,\n",
    "    sentences_length)\n",
    "test_list_of_token_ids = convert_tokens_to_ids(\n",
    "    tokenized_sentences_test,\n",
    "    id_to_word,\n",
    "    embedding_word_dict,\n",
    "    sentences_length)\n",
    "X_train = np.array(train_list_of_token_ids)\n",
    "X_test = np.array(test_list_of_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_model_func = lambda: get_model(\n",
    "    embedding_matrix,\n",
    "    sentences_length,\n",
    "    dropout_rate,\n",
    "    recurrent_units,\n",
    "    dense_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_data, test_data, list_sentences_train, list_sentences_test\n",
    "del tokenized_sentences_train, tokenized_sentences_test, words_dict\n",
    "del embedding_list, embedding_word_dict\n",
    "del train_list_of_token_ids, test_list_of_token_ids\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Starting to train models...\")\n",
    "models = train_folds(X_train, y_train, X_test, fold_count, batch_size, get_model_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提交\n",
    "\n",
    "使用默认参数以 10 折训练模型。使用平均排名进行提交。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "LABELS = [\"project_is_approved\"]\n",
    "\n",
    "base = \"../input/donorschooseorg-application-screening-predictions/predictions/predictions/\"\n",
    "predict_list = []\n",
    "for j in range(10):\n",
    "    predict_list.append(np.load(base + \"predictions_001/test_predicts%d.npy\"%j))\n",
    "    \n",
    "print(\"Rank averaging on \", len(predict_list), \" files\")\n",
    "predcitions = np.zeros_like(predict_list[0])\n",
    "for predict in predict_list:\n",
    "    predcitions = np.add(predcitions.flatten(), rankdata(predict)/predcitions.shape[0])  \n",
    "predcitions /= len(predict_list)\n",
    "\n",
    "submission = pd.read_csv('../input/donorschoose-application-screening/sample_submission.csv')\n",
    "submission[LABELS] = predcitions\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论\n",
    "\n",
    "我们可以看到，胶囊网络有助于文本分类。即使没有任何超参数调整，也可以使用胶囊网络建立强大的基准。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-capsule-networks/notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
